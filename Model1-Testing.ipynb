{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from numpy import mean, argmax\n",
    "\n",
    "import train\n",
    "import loadDataset\n",
    "import model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (10962, 64, 64, 3) Y: (10962,)\n"
     ]
    }
   ],
   "source": [
    "X, Y = loadDataset.loadDataSet('./dataset/imgs/rock_frames', './dataset/imgs/paper_frames',\n",
    "                               './dataset/imgs/scissor_frames', './dataset/csvs/rock.csv',\n",
    "                               './dataset/csvs/paper.csv', './dataset/csvs/scissor.csv')\n",
    "X = (X.astype(np.float32) - 128) / 128  # Normalize [-1,1]\n",
    "Y = (Y > 0).astype(int)  # 0: None/NA, 1: Rock/Paper/Scissors\n",
    "\n",
    "print(\"X:\", X.shape, \"Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Cross-Validation Function\n",
    "(N-45) 45-frame videos are too many to load all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_len = 45\n",
    "\n",
    "# Custom Batch Generator\n",
    "def _batches_video(inputs, batch_size, shuffle=False, allow_smaller_final_batch=False):\n",
    "    if not isinstance(inputs, list) and not isinstance(inputs, tuple):\n",
    "        raise TypeError(\"Inputs must be of type list or tuple.\")\n",
    "    if not all([isinstance(x, np.ndarray) for x in inputs]):\n",
    "        raise TypeError(\"Each input in the input list must be a numpy array.\")\n",
    "    total_size = inputs[0].shape[0]\n",
    "    if not all([x.shape[0] == total_size for x in inputs]):\n",
    "        raise RuntimeError(\"All inputs must have equal first dimension.\")\n",
    "\n",
    "    total_size -= video_len\n",
    "    order = np.arange(total_size) if shuffle is False else np.random.permutation(total_size)\n",
    "    for i in range(int(total_size / batch_size)):\n",
    "        start_indices = order[(i)*batch_size:(i+1)*batch_size]                    \n",
    "        yield [np.concatenate([x[None, (start):(start+video_len)] for start in start_indices]) for x in inputs]\n",
    "    if allow_smaller_final_batch:\n",
    "        start_indices = order[int(total_size / batch_size)*batch_size:]                    \n",
    "        yield [np.concatenate([x[None, (start):(start+video_len)] for start in start_indices]) for x in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Cross-Validation that uses Custom Batch Generator\n",
    "def cross_validation(session, model, x, y, batch_size=64, epochs=5, K=5, shuffle=False, verbose=True, print_interval=100):\n",
    "    # https://stackoverflow.com/questions/39748660/how-to-perform-k-fold-cross-validation-with-tensorflow\n",
    "    [predict_op, loss_op, accuracy_op, train_op], (X, Y, training) = model\n",
    "\n",
    "    # K-Fold Loop\n",
    "    train_loss, train_accuracy = [], []\n",
    "    valid_loss, valid_accuracy = [], []\n",
    "    k = 0\n",
    "    for train_i, valid_i in KFold(n_splits=K).split(x):\n",
    "        train_loss.append([])\n",
    "        train_accuracy.append([])\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.local_variables_initializer())\n",
    "\n",
    "        num_train_batches = int(x[train_i].shape[0] / batch_size)\n",
    "        num_valid_batches = int(x[valid_i].shape[0] / batch_size)\n",
    "\n",
    "        # Training\n",
    "        for e in range(epochs):\n",
    "            sum_loss, sum_accuracy = 0, 0\n",
    "            for batch_i, (batch_x, batch_y) in enumerate(_batches_video([x[train_i], y[train_i]], batch_size=batch_size, shuffle=shuffle, allow_smaller_final_batch=False)):\n",
    "                batch_y = batch_y[:, -1]\n",
    "                loss, accuracy, _ = session.run([loss_op, accuracy_op, train_op],\n",
    "                                                feed_dict={X: batch_x, Y: batch_y, training: True})\n",
    "\n",
    "                if verbose and batch_i % print_interval == 0:\n",
    "                    print(\"Train Batch {}: Loss = {}, Accuracy = {}\".format(batch_i + 1, loss, accuracy))\n",
    "                sum_loss += loss\n",
    "                sum_accuracy += accuracy\n",
    "\n",
    "            train_loss[k].append(sum_loss / num_train_batches)\n",
    "            train_accuracy[k].append(sum_accuracy / num_train_batches)\n",
    "            if verbose:\n",
    "                print(\"Epoch {}: Average Train Loss = {}, Average Train Accuracy = {}\\n\"\n",
    "                      .format(e + 1, train_loss[k][e], train_accuracy[k][e]))\n",
    "\n",
    "        # Validation\n",
    "        sum_loss, sum_accuracy = 0, 0\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(_batches_video([x[valid_i], y[valid_i]], batch_size=batch_size, shuffle=shuffle, allow_smaller_final_batch=False)):\n",
    "            batch_y = batch_y[:, -1]\n",
    "            loss, accuracy = session.run([loss_op, accuracy_op],\n",
    "                                         feed_dict={X: batch_x, Y: batch_y, training: False})\n",
    "\n",
    "            if verbose and batch_i % print_interval == 0:\n",
    "                print(\"Valid Batch {}: Loss = {}, Accuracy = {}\".format(batch_i + 1, loss, accuracy))\n",
    "            sum_loss += loss\n",
    "            sum_accuracy += accuracy\n",
    "\n",
    "        valid_loss.append(sum_loss / num_valid_batches)\n",
    "        valid_accuracy.append(sum_accuracy / num_valid_batches)\n",
    "        if verbose:\n",
    "            print(\"Fold {}: Validation Loss = {}, Validation Accuracy = {}\\n\"\n",
    "                  .format(k + 1, valid_loss[k], valid_accuracy[k]))\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    # Results\n",
    "    print(\"Average Valid Loss = {}, Average Valid Accuracy = {}\".format(mean(valid_loss), mean(valid_accuracy)))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "    plt.plot(np.arange(epochs), np.array(train_loss).T)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(np.arange(epochs), np.arange(epochs))\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Fold %d\" % i for i in range(1, K+1)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = _batches_video([X, Y], batch_size=64, shuffle=True, allow_smaller_final_batch=False)\n",
    "for Xb, Yb in test_batches:\n",
    "    Xt, Yt = tf.placeholder(tf.float32, (None, 45, 64, 64, 3)), tf.placeholder(tf.int64, None)\n",
    "    new_shape = [-1]+[d for i, d in enumerate(Xt.shape[1:]) if i != 1-1]\n",
    "    \n",
    "    conc = []\n",
    "    for i in range(Xt.shape[1]):\n",
    "        Xt2 = tf.reshape(Xt[:, i], new_shape)\n",
    "        l = tf.layers.conv2d(Xt2, filters=1, kernel_size=[7,7], strides=[4,4], padding=\"valid\")\n",
    "        conc.append(tf.expand_dims(l, 1))\n",
    "    print(tf.concat(conc, axis=1))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = tf.placeholder(tf.float32, (None, 45, 64, 64, 3))\n",
    "x1 = tf.reshape(Xt[:, 0], (-1, 64, 64, 3))\n",
    "x2 = tf.reshape(Xt[:, 1], (-1, 64, 64, 3))\n",
    "print(Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Tensorflow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:691: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = model1.construct(X.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch 1: Loss = 0.7310421466827393, Accuracy = 0.484375\n",
      "Train Batch 11: Loss = 0.5983849763870239, Accuracy = 0.625\n",
      "Train Batch 21: Loss = 0.42891061305999756, Accuracy = 0.859375\n",
      "Train Batch 31: Loss = 0.3024279475212097, Accuracy = 0.890625\n",
      "Train Batch 41: Loss = 0.18213023245334625, Accuracy = 0.890625\n",
      "Train Batch 51: Loss = 0.20979221165180206, Accuracy = 0.890625\n",
      "Train Batch 61: Loss = 0.11843259632587433, Accuracy = 0.9375\n",
      "Train Batch 71: Loss = 0.18731680512428284, Accuracy = 0.9375\n",
      "Train Batch 81: Loss = 0.15587764978408813, Accuracy = 0.9375\n",
      "Train Batch 91: Loss = 0.22917750477790833, Accuracy = 0.921875\n",
      "Train Batch 101: Loss = 0.09380890429019928, Accuracy = 0.984375\n",
      "Train Batch 111: Loss = 0.22687065601348877, Accuracy = 0.875\n",
      "Train Batch 121: Loss = 0.1690642535686493, Accuracy = 0.9375\n",
      "Train Batch 131: Loss = 0.26629316806793213, Accuracy = 0.90625\n",
      "Epoch 1: Average Train Loss = 0.2712198434955012, Average Train Accuracy = 0.8731751824817519\n",
      "\n",
      "Train Batch 1: Loss = 0.13069719076156616, Accuracy = 0.953125\n",
      "Train Batch 11: Loss = 0.14686743915081024, Accuracy = 0.96875\n",
      "Train Batch 21: Loss = 0.11281993985176086, Accuracy = 0.9375\n",
      "Train Batch 31: Loss = 0.2031613290309906, Accuracy = 0.9375\n",
      "Train Batch 41: Loss = 0.12478048354387283, Accuracy = 0.953125\n",
      "Train Batch 51: Loss = 0.039279818534851074, Accuracy = 1.0\n",
      "Train Batch 61: Loss = 0.07574423402547836, Accuracy = 0.96875\n",
      "Train Batch 71: Loss = 0.16425122320652008, Accuracy = 0.953125\n",
      "Train Batch 81: Loss = 0.1183113306760788, Accuracy = 0.953125\n",
      "Train Batch 91: Loss = 0.18227258324623108, Accuracy = 0.9375\n",
      "Train Batch 101: Loss = 0.09530210494995117, Accuracy = 0.953125\n",
      "Train Batch 111: Loss = 0.08214275538921356, Accuracy = 0.96875\n",
      "Train Batch 121: Loss = 0.123788982629776, Accuracy = 0.953125\n",
      "Train Batch 131: Loss = 0.12512065470218658, Accuracy = 0.953125\n",
      "Epoch 2: Average Train Loss = 0.12763947097543818, Average Train Accuracy = 0.9470802919708029\n",
      "\n",
      "Train Batch 1: Loss = 0.06607508659362793, Accuracy = 0.96875\n",
      "Train Batch 11: Loss = 0.0886441320180893, Accuracy = 0.96875\n",
      "Train Batch 21: Loss = 0.0965869128704071, Accuracy = 0.9375\n",
      "Train Batch 31: Loss = 0.12012799084186554, Accuracy = 0.921875\n",
      "Train Batch 41: Loss = 0.10675310343503952, Accuracy = 0.96875\n",
      "Train Batch 51: Loss = 0.0474226288497448, Accuracy = 0.984375\n",
      "Train Batch 61: Loss = 0.0757569670677185, Accuracy = 0.96875\n",
      "Train Batch 71: Loss = 0.03642563149333, Accuracy = 1.0\n",
      "Train Batch 81: Loss = 0.03171033412218094, Accuracy = 0.984375\n",
      "Train Batch 91: Loss = 0.052682459354400635, Accuracy = 0.984375\n",
      "Train Batch 101: Loss = 0.04621411859989166, Accuracy = 0.984375\n",
      "Train Batch 111: Loss = 0.17858801782131195, Accuracy = 0.9375\n",
      "Train Batch 121: Loss = 0.02532918192446232, Accuracy = 0.984375\n",
      "Train Batch 131: Loss = 0.011891359463334084, Accuracy = 1.0\n",
      "Epoch 3: Average Train Loss = 0.09748535240272971, Average Train Accuracy = 0.9572308394160584\n",
      "\n",
      "Train Batch 1: Loss = 0.19780617952346802, Accuracy = 0.9375\n",
      "Train Batch 11: Loss = 0.17476233839988708, Accuracy = 0.9375\n",
      "Train Batch 21: Loss = 0.11279608309268951, Accuracy = 0.921875\n",
      "Train Batch 31: Loss = 0.15928387641906738, Accuracy = 0.96875\n",
      "Train Batch 41: Loss = 0.06294463574886322, Accuracy = 0.96875\n",
      "Train Batch 51: Loss = 0.017402468249201775, Accuracy = 1.0\n",
      "Train Batch 61: Loss = 0.13062450289726257, Accuracy = 0.96875\n",
      "Train Batch 71: Loss = 0.030757121741771698, Accuracy = 1.0\n",
      "Train Batch 81: Loss = 0.030497878789901733, Accuracy = 1.0\n",
      "Train Batch 91: Loss = 0.080678291618824, Accuracy = 0.96875\n",
      "Train Batch 101: Loss = 0.028033114969730377, Accuracy = 0.984375\n",
      "Train Batch 111: Loss = 0.030854159966111183, Accuracy = 1.0\n",
      "Train Batch 121: Loss = 0.08613602817058563, Accuracy = 0.96875\n",
      "Train Batch 131: Loss = 0.06384004652500153, Accuracy = 0.96875\n",
      "Epoch 4: Average Train Loss = 0.09131655020870431, Average Train Accuracy = 0.9608804744525548\n",
      "\n",
      "Train Batch 1: Loss = 0.0324273519217968, Accuracy = 0.984375\n",
      "Train Batch 11: Loss = 0.07313324511051178, Accuracy = 0.984375\n",
      "Train Batch 21: Loss = 0.12019598484039307, Accuracy = 0.984375\n",
      "Train Batch 31: Loss = 0.04956691712141037, Accuracy = 0.984375\n",
      "Train Batch 41: Loss = 0.12852522730827332, Accuracy = 0.953125\n",
      "Train Batch 51: Loss = 0.039377450942993164, Accuracy = 0.984375\n",
      "Train Batch 61: Loss = 0.0647236630320549, Accuracy = 0.953125\n",
      "Train Batch 71: Loss = 0.12314793467521667, Accuracy = 0.96875\n",
      "Train Batch 81: Loss = 0.11535216867923737, Accuracy = 0.96875\n",
      "Train Batch 91: Loss = 0.02642842009663582, Accuracy = 1.0\n",
      "Train Batch 101: Loss = 0.013560155406594276, Accuracy = 1.0\n",
      "Train Batch 111: Loss = 0.1111808717250824, Accuracy = 0.984375\n",
      "Train Batch 121: Loss = 0.09571442008018494, Accuracy = 0.953125\n",
      "Train Batch 131: Loss = 0.04691024869680405, Accuracy = 0.984375\n",
      "Epoch 5: Average Train Loss = 0.07427405292698502, Average Train Accuracy = 0.9660127737226277\n",
      "\n",
      "Valid Batch 1: Loss = 0.106584832072258, Accuracy = 0.96875\n",
      "Valid Batch 11: Loss = 0.1726837456226349, Accuracy = 0.921875\n",
      "Valid Batch 21: Loss = 0.23809461295604706, Accuracy = 0.859375\n",
      "Valid Batch 31: Loss = 0.34161025285720825, Accuracy = 0.90625\n"
     ]
    }
   ],
   "source": [
    "cross_validation(session, model, X, Y, epochs=5, shuffle=True, print_interval=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
