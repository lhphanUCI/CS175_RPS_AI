{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import KFold\n",
    "from numpy import mean, argmax\n",
    "\n",
    "import loadDataset\n",
    "import models\n",
    "import augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No longer shrinking images. This will crash your computer.\n",
    "#X, Y = loadDataset.loadDataSet('./dataset/imgs/rock_frames', './dataset/imgs/paper_frames',\n",
    "#                               './dataset/imgs/scissor_frames', './dataset/csvs/rock.csv',\n",
    "#                               './dataset/csvs/paper.csv', './dataset/csvs/scissor.csv')\n",
    "#X = X.astype(np.float32) / 255\n",
    "\n",
    "#X1, Y1 = X, (Y > 0).astype(int)  # 0: None/NA, 1: Rock/Paper/Scissors\n",
    "#X2, Y2 = X[Y > 0], Y[Y > 0] - 1  # Y is 0-based containing only Rock, Paper, or Scissors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _csv_to_list(path):\n",
    "    with open(path) as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "\n",
    "def _load_images(paths: str or list or tuple) -> np.array:\n",
    "    if isinstance(paths, str):\n",
    "        paths = [paths]\n",
    "    imgs = np.array([cv2.cvtColor(cv2.imread(path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB) for path in paths]).astype(float) / 255\n",
    "    return imgs if imgs.shape[0] > 1 else imgs.reshape(imgs.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_batch_generator_augment(img_paths, csv_paths, session, batch_size=64, vid_length=45, shuffle=False):\n",
    "    if not isinstance(img_paths, (tuple, list)):\n",
    "        img_paths = [img_paths]\n",
    "    if not isinstance(csv_paths, (tuple, list)):\n",
    "        csv_paths = [csv_paths]\n",
    "        \n",
    "    csv_files = [[int(x) for x in _csv_to_list(path)] for path in csv_paths]\n",
    "    img_files = [[path + \"/\" + str(i) + \".jpeg\" for i in range(1, len(csv_files[j]) + 1)] for j, path in enumerate(img_paths)]\n",
    "    csv_files = np.concatenate(np.array(csv_files))\n",
    "    img_files = [item for sublist in img_files for item in sublist]  # Flatten to 1D\n",
    "    \n",
    "    img_files, csv_files = [f for i, f in enumerate(img_files) if csv_files[i] > 0], csv_files[csv_files > 0] - 1  # Y is 0-based containing only Rock, Paper, or Scissors.\n",
    "    total_length = len(csv_files) - vid_length  # Number of video starting points\n",
    "\n",
    "    image_shape = _load_images(img_files[0]).shape\n",
    "    oper = augment._augment_video_oper(img_size=image_shape, vid_length=vid_length)\n",
    "    order = np.random.permutation(total_length) if shuffle is True else np.arange(total_length)\n",
    "    \n",
    "    for i in range(int(total_length / batch_size)):\n",
    "        start_indices = order[(i)*batch_size:(i+1)*batch_size]\n",
    "        print(start_indices)\n",
    "        x_batch = [_load_images(img_files[(start):(start+vid_length)]) for start in start_indices]\n",
    "        y_batch = [csv_files[(start):(start+vid_length)] for start in start_indices]\n",
    "\n",
    "        for j, x_vid in enumerate(x_batch):\n",
    "            x_batch[j] = augment._augment_video(x_vid, session, oper)\n",
    "\n",
    "        yield np.stack(x_batch), np.stack(y_batch)[:, -1]\n",
    "        \n",
    "        \n",
    "def image_batch_generator_augment(img_paths, csv_paths, session, batch_size=64, shuffle=False):\n",
    "    if not isinstance(img_paths, (tuple, list)):\n",
    "        img_paths = [img_paths]\n",
    "    if not isinstance(csv_paths, (tuple, list)):\n",
    "        csv_paths = [csv_paths]\n",
    "        \n",
    "    csv_files = [[int(x) for x in _csv_to_list(path)] for path in csv_paths]\n",
    "    img_files = [[path + \"/\" + str(i) + \".jpeg\" for i in range(1, len(csv_files[j]) + 1)] for j, path in enumerate(img_paths)]\n",
    "    csv_files = np.concatenate(np.array(csv_files))\n",
    "    img_files = [item for sublist in img_files for item in sublist]  # Flatten to 1D\n",
    "    \n",
    "    img_files, csv_files = img_files, (csv_files > 0).astype(int)  # 0: None/NA, 1: Rock/Paper/Scissors\n",
    "    total_length = len(csv_files)  # Number of images\n",
    "\n",
    "    image_shape = _load_images(img_files[0]).shape\n",
    "    oper = augment._augment_video_oper(img_size=image_shape, vid_length=vid_length)\n",
    "    order = np.random.permutation(total_length) if shuffle is True else np.arange(total_length)\n",
    "    \n",
    "    for i in range(int(total_length / batch_size)):\n",
    "        indices = order[(i)*batch_size:(i+1)*batch_size]\n",
    "        x_batch = [_load_images(img_files[j]) for j in indices]\n",
    "        y_batch = csv_files[indices]\n",
    "\n",
    "        for j, x_img in enumerate(x_batch):\n",
    "            x_batch[j] = _augment_video(x_img[None, :], session, oper)\n",
    "\n",
    "        yield np.stack(x_batch), y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit1(session, model, img_paths, csv_paths, batch_size=64, epochs=5, shuffle=False, verbose=True, print_interval=100):\n",
    "    [predict_op, loss_op, accuracy_op, train_op], (X, Y, training) = model\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.local_variables_initializer())\n",
    "    \n",
    "    train_loss, train_accuracy = [], []\n",
    "    valid_loss, valid_accuracy = [], []\n",
    "    \n",
    "    # Training\n",
    "    for e in range(epochs):\n",
    "        sum_loss, sum_accuracy, num_batches = 0, 0, 0\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(video_batch_generator_augment(img_paths, csv_paths, session, batch_size=batch_size, vid_length=45, shuffle=shuffle)):\n",
    "            loss, accuracy, _ = session.run([loss_op, accuracy_op, train_op],\n",
    "                                            feed_dict={X: batch_x, Y: batch_y, training: True})\n",
    "\n",
    "            if verbose and batch_i % print_interval == 0:\n",
    "                print(\"Train Batch {}: Loss = {}, Accuracy = {}\".format(batch_i, loss, accuracy))\n",
    "            sum_loss += loss\n",
    "            sum_accuracy += accuracy\n",
    "            num_batches += 1\n",
    "\n",
    "        train_loss.append(sum_loss / num_batches)\n",
    "        train_accuracy.append(sum_accuracy / num_batches)\n",
    "        if verbose:\n",
    "            print(\"Epoch {}: Average Train Loss = {}, Average Train Accuracy = {}\\n\"\n",
    "                  .format(e + 1, train_loss[e], train_accuracy[e]))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "    plt.plot(np.arange(epochs), np.array(train_loss), label=\"Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(np.arange(epochs), np.arange(epochs))\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def fit2(session, model, img_paths, csv_paths, batch_size=64, epochs=5, shuffle=False, verbose=True, print_interval=100):\n",
    "    [predict_op, loss_op, accuracy_op, train_op], (X, Y, training) = model\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.local_variables_initializer())\n",
    "    \n",
    "    train_loss, train_accuracy = [], []\n",
    "    valid_loss, valid_accuracy = [], []\n",
    "    \n",
    "    # Training\n",
    "    for e in range(epochs):\n",
    "        sum_loss, sum_accuracy, num_batches = 0, 0, 0\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(image_batch_generator_augment(img_paths, csv_paths, session, batch_size=batch_size, shuffle=shuffle)):\n",
    "            loss, accuracy, _ = session.run([loss_op, accuracy_op, train_op],\n",
    "                                            feed_dict={X: batch_x, Y: batch_y, training: True})\n",
    "\n",
    "            if verbose and batch_i % print_interval == 0:\n",
    "                print(\"Train Batch {}: Loss = {}, Accuracy = {}\".format(batch_i, loss, accuracy))\n",
    "            sum_loss += loss\n",
    "            sum_accuracy += accuracy\n",
    "            num_batches += 1\n",
    "\n",
    "        train_loss.append(sum_loss / num_batches)\n",
    "        train_accuracy.append(sum_accuracy / num_batches)\n",
    "        if verbose:\n",
    "            print(\"Epoch {}: Average Train Loss = {}, Average Train Accuracy = {}\\n\"\n",
    "                  .format(e + 1, train_loss[e], train_accuracy[e]))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "    plt.plot(np.arange(epochs), np.array(train_loss), label=\"Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(np.arange(epochs), np.arange(epochs))\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as session:\n",
    "    image_size = [64, 64, 3]\n",
    "    img_paths = [\"./dataset/imgs/rock_frames\", \"./dataset/imgs/paper_frames\", \"./dataset/imgs/scissor_frames\"]\n",
    "    csv_paths = [\"./dataset/csvs/rock.csv\", \"./dataset/csvs/paper.csv\", \"./dataset/csvs/scissor.csv\"]\n",
    "    model1 = models.model1(image_size, image_history_length=45)\n",
    "    fit1(session, model1, img_paths, csv_paths, epochs=5, shuffle=True, verbose=True, print_interval=10)\n",
    "    model2 = models.model2(image_size)\n",
    "    fit2(session, model2, img_paths, csv_paths, epochs=2, shuffle=True, verbose=True, print_interval=10)\n",
    "    \n",
    "    # Save Model\n",
    "    # https://www.tensorflow.org/programmers_guide/saved_model\n",
    "    save_path = os.path.join(os.getcwd(), \"savedmodels\\\\both\\\\models.ckpt\")\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session, save_path)\n",
    "    print(\"Session Saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Cross-Validation that uses Custom Batch Generator\n",
    "def cross_validation1(session, model, x, y, batch_size=64, epochs=5, K=5, shuffle=False, verbose=True, print_interval=100):\n",
    "    # https://stackoverflow.com/questions/39748660/how-to-perform-k-fold-cross-validation-with-tensorflow\n",
    "    [predict_op, loss_op, accuracy_op, train_op], (X, Y, training) = model\n",
    "\n",
    "    # K-Fold Loop\n",
    "    train_loss, train_accuracy = [], []\n",
    "    valid_loss, valid_accuracy = [], []\n",
    "    k = 0\n",
    "    for train_i, valid_i in KFold(n_splits=K).split(x):\n",
    "        train_loss.append([])\n",
    "        train_accuracy.append([])\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.local_variables_initializer())\n",
    "\n",
    "        num_train_batches = int(x[train_i].shape[0] / batch_size)\n",
    "        num_valid_batches = int(x[valid_i].shape[0] / batch_size)\n",
    "\n",
    "        # Training\n",
    "        for e in range(epochs):\n",
    "            sum_loss, sum_accuracy = 0, 0\n",
    "            for batch_i, (batch_x, batch_y) in enumerate(_batches_video([x[train_i], y[train_i]], batch_size=batch_size, shuffle=shuffle, allow_smaller_final_batch=False)):\n",
    "                batch_y = batch_y[:, -1]\n",
    "                loss, accuracy, _ = session.run([loss_op, accuracy_op, train_op],\n",
    "                                                feed_dict={X: batch_x, Y: batch_y, training: True})\n",
    "\n",
    "                if verbose and batch_i % print_interval == 0:\n",
    "                    print(\"Train Batch {}: Loss = {}, Accuracy = {}\".format(batch_i, loss, accuracy))\n",
    "                sum_loss += loss\n",
    "                sum_accuracy += accuracy\n",
    "\n",
    "            train_loss[k].append(sum_loss / num_train_batches)\n",
    "            train_accuracy[k].append(sum_accuracy / num_train_batches)\n",
    "            if verbose:\n",
    "                print(\"Epoch {}: Average Train Loss = {}, Average Train Accuracy = {}\\n\"\n",
    "                      .format(e + 1, train_loss[k][e], train_accuracy[k][e]))\n",
    "\n",
    "        # Validation\n",
    "        sum_loss, sum_accuracy = 0, 0\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(_batches_video([x[valid_i], y[valid_i]], batch_size=batch_size, shuffle=shuffle, allow_smaller_final_batch=False)):\n",
    "            batch_y = batch_y[:, -1]\n",
    "            loss, accuracy = session.run([loss_op, accuracy_op],\n",
    "                                         feed_dict={X: batch_x, Y: batch_y, training: False})\n",
    "\n",
    "            if verbose and batch_i % print_interval == 0:\n",
    "                print(\"Valid Batch {}: Loss = {}, Accuracy = {}\".format(batch_i, loss, accuracy))\n",
    "            sum_loss += loss\n",
    "            sum_accuracy += accuracy\n",
    "\n",
    "        valid_loss.append(sum_loss / num_valid_batches)\n",
    "        valid_accuracy.append(sum_accuracy / num_valid_batches)\n",
    "        if verbose:\n",
    "            print(\"Fold {}: Validation Loss = {}, Validation Accuracy = {}\\n\"\n",
    "                  .format(k + 1, valid_loss[k], valid_accuracy[k]))\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    # Results\n",
    "    print(\"Average Valid Loss = {}, Average Valid Accuracy = {}\".format(mean(valid_loss), mean(valid_accuracy)))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "    plt.plot(np.arange(epochs), np.array(train_loss).T)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(np.arange(epochs), np.arange(epochs))\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Fold %d\" % i for i in range(1, K+1)])\n",
    "    plt.show()\n",
    "    \n",
    "def cross_validation2(session, model, x, y, batch_size=64, epochs=5, K=5, shuffle=False, verbose=True, print_interval=100):\n",
    "    # https://stackoverflow.com/questions/39748660/how-to-perform-k-fold-cross-validation-with-tensorflow\n",
    "    [predict_op, loss_op, accuracy_op, train_op], (X, Y, training) = model\n",
    "\n",
    "    # K-Fold Loop\n",
    "    train_loss, train_accuracy = [], []\n",
    "    valid_loss, valid_accuracy = [], []\n",
    "    k = 0\n",
    "    for train_i, valid_i in KFold(n_splits=K).split(x):\n",
    "        train_loss.append([])\n",
    "        train_accuracy.append([])\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.local_variables_initializer())\n",
    "\n",
    "        num_train_batches = int(x[train_i].shape[0] / batch_size)\n",
    "        num_valid_batches = int(x[valid_i].shape[0] / batch_size)\n",
    "\n",
    "        # Training\n",
    "        for e in range(epochs):\n",
    "            sum_loss, sum_accuracy = 0, 0\n",
    "            for batch_i, (batch_x, batch_y) in enumerate(_batches([x[train_i], y[train_i]], batch_size=batch_size, shuffle=shuffle, allow_smaller_final_batch=False)):\n",
    "                loss, accuracy, _ = session.run([loss_op, accuracy_op, train_op],\n",
    "                                                feed_dict={X: batch_x, Y: batch_y, training: True})\n",
    "\n",
    "                if verbose and batch_i % print_interval == 0:\n",
    "                    print(\"Train Batch {}: Loss = {}, Accuracy = {}\".format(batch_i + 1, loss, accuracy))\n",
    "                sum_loss += loss\n",
    "                sum_accuracy += accuracy\n",
    "\n",
    "            train_loss[k].append(sum_loss / num_train_batches)\n",
    "            train_accuracy[k].append(sum_accuracy / num_train_batches)\n",
    "            if verbose:\n",
    "                print(\"Epoch {}: Average Train Loss = {}, Average Train Accuracy = {}\\n\"\n",
    "                      .format(e + 1, train_loss[k][e], train_accuracy[k][e]))\n",
    "\n",
    "        # Validation\n",
    "        sum_loss, sum_accuracy = 0, 0\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(_batches([x[valid_i], y[valid_i]], batch_size=batch_size, shuffle=shuffle, allow_smaller_final_batch=False)):\n",
    "            loss, accuracy = session.run([loss_op, accuracy_op],\n",
    "                                         feed_dict={X: batch_x, Y: batch_y, training: False})\n",
    "\n",
    "            if verbose and batch_i % print_interval == 0:\n",
    "                print(\"Valid Batch {}: Loss = {}, Accuracy = {}\".format(batch_i + 1, loss, accuracy))\n",
    "            sum_loss += loss\n",
    "            sum_accuracy += accuracy\n",
    "\n",
    "        valid_loss.append(sum_loss / num_valid_batches)\n",
    "        valid_accuracy.append(sum_accuracy / num_valid_batches)\n",
    "        if verbose:\n",
    "            print(\"Fold {}: Validation Loss = {}, Validation Accuracy = {}\\n\"\n",
    "                  .format(k + 1, valid_loss[k], valid_accuracy[k]))\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    # Results\n",
    "    print(\"Average Valid Loss = {}, Average Valid Accuracy = {}\".format(mean(valid_loss), mean(valid_accuracy)))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "    plt.plot(np.arange(epochs), np.array(train_loss).T)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(np.arange(epochs), np.arange(epochs))\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Fold %d\" % i for i in range(1, K+1)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as session:\n",
    "    image_shape = X1.shape[1:]\n",
    "    model1 = models.model1(image_shape, 45)\n",
    "    cross_validation1(session, model1, X1, Y1, epochs=5, shuffle=True, print_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as session:\n",
    "    image_shape = X2.shape[1:]\n",
    "    mode2 = models.model2(image_shape)\n",
    "    cross_validation2(session, model2, X2, Y2, epochs=5, shuffle=True, print_interval=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
