{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kyle\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import KFold\n",
    "from numpy import mean, argmax\n",
    "\n",
    "import loadDataset\n",
    "import models\n",
    "import augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = loadDataset.loadDataSet('./dataset/imgs/rock_frames', './dataset/imgs/paper_frames',\n",
    "                               './dataset/imgs/scissor_frames', './dataset/csvs/rock.csv',\n",
    "                               './dataset/csvs/paper.csv', './dataset/csvs/scissor.csv')\n",
    "X = np.stack([cv2.Canny(X[i], 50, 100) for i in range(X.shape[0])])  # Apply Canny Edge Detection\n",
    "X = np.expand_dims(X, 3)  # Make images 3D for tensorflow ops.\n",
    "\n",
    "X1, Y1 = X, (Y > 0).astype(int)  # 0: None/NA, 1: Rock/Paper/Scissors\n",
    "X2, Y2 = X[Y > 0], Y[Y > 0] - 1  # Y is 0-based containing only Rock, Paper, or Scissors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_len = 45\n",
    "\n",
    "# Custom Batch Generator\n",
    "def _batches_video(inputs, batch_size, shuffle=False, allow_smaller_final_batch=False):\n",
    "    if not isinstance(inputs, list) and not isinstance(inputs, tuple):\n",
    "        raise TypeError(\"Inputs must be of type list or tuple.\")\n",
    "    if not all([isinstance(x, np.ndarray) for x in inputs]):\n",
    "        raise TypeError(\"Each input in the input list must be a numpy array.\")\n",
    "    total_size = inputs[0].shape[0]\n",
    "    if not all([x.shape[0] == total_size for x in inputs]):\n",
    "        raise RuntimeError(\"All inputs must have equal first dimension.\")\n",
    "\n",
    "    total_size -= video_len\n",
    "    order = np.arange(total_size) if shuffle is False else np.random.permutation(total_size)\n",
    "    for i in range(int(total_size / batch_size)):\n",
    "        start_indices = order[(i)*batch_size:(i+1)*batch_size]                    \n",
    "        yield [np.concatenate([x[None, (start):(start+video_len)] for start in start_indices]) for x in inputs]\n",
    "    if allow_smaller_final_batch:\n",
    "        start_indices = order[int(total_size / batch_size)*batch_size:]                    \n",
    "        yield [np.concatenate([x[None, (start):(start+video_len)] for start in start_indices]) for x in inputs]\n",
    "        \n",
    "def _batches(inputs, batch_size, shuffle=False, allow_smaller_final_batch=False):\n",
    "    if not isinstance(inputs, list) and not isinstance(inputs, tuple):\n",
    "        raise TypeError(\"Inputs must be of type list or tuple.\")\n",
    "    if not all([isinstance(x, np.ndarray) for x in inputs]):\n",
    "        raise TypeError(\"Each input in the input list must be a numpy array.\")\n",
    "    total_size = inputs[0].shape[0]\n",
    "    if not all([x.shape[0] == total_size for x in inputs]):\n",
    "        raise RuntimeError(\"All inputs must have equal first dimension.\")\n",
    "\n",
    "    order = np.arange(total_size) if shuffle is False else np.random.permutation(total_size)\n",
    "    for i in range(int(total_size / batch_size)):\n",
    "        yield [x[order[(i)*batch_size:(i+1)*batch_size]] for x in inputs]\n",
    "    if allow_smaller_final_batch:\n",
    "        yield [x[order[int(total_size / batch_size)*batch_size:]] for x in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit1(session, model, x, y, batch_size=64, epochs=5, shuffle=False, verbose=True, print_interval=100):    \n",
    "    [predict_op, loss_op, accuracy_op, train_op], (X, Y, training) = model\n",
    "    augment_op = augment._augment_video_oper(img_size=[80, 80, 1], vid_length=45)\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.local_variables_initializer())\n",
    "    \n",
    "    train_loss, train_accuracy = [], []\n",
    "    valid_loss, valid_accuracy = [], []\n",
    "    \n",
    "    num_train_batches = int(x.shape[0] / batch_size)\n",
    "    \n",
    "    # Training\n",
    "    print(\"Beginning Training...\")\n",
    "    for e in range(epochs):        \n",
    "        sum_loss, sum_accuracy = 0, 0\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(_batches_video([x, y], batch_size=batch_size, shuffle=shuffle, allow_smaller_final_batch=False)):\n",
    "            batch_x = np.stack([augment.augment_video(batch_x[i], session, augment_op) for i in range(batch_size)])\n",
    "            batch_y = batch_y[:, -1]\n",
    "                \n",
    "            loss, accuracy, _ = session.run([loss_op, accuracy_op, train_op],\n",
    "                                            feed_dict={X: batch_x, Y: batch_y, training: True})\n",
    "\n",
    "            if verbose and batch_i % print_interval == 0:\n",
    "                print(\"Train Batch {}: Loss = {}, Accuracy = {}\".format(batch_i, loss, accuracy))\n",
    "            sum_loss += loss\n",
    "            sum_accuracy += accuracy\n",
    "\n",
    "        train_loss.append(sum_loss / num_train_batches)\n",
    "        train_accuracy.append(sum_accuracy / num_train_batches)\n",
    "        if verbose:\n",
    "            print(\"Epoch {}: Average Train Loss = {}, Average Train Accuracy = {}\\n\"\n",
    "                  .format(e + 1, train_loss[e], train_accuracy[e]))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "    plt.plot(np.arange(epochs), np.array(train_loss), label=\"Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(np.arange(epochs), np.arange(epochs))\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def fit2(session, model, x, y, batch_size=64, epochs=5, shuffle=False, verbose=True, print_interval=100):\n",
    "    [predict_op, loss_op, accuracy_op, train_op], (X, Y, training) = model\n",
    "    augment_op = augment._augment_video_oper(img_size=[80, 80, 1], vid_length=1)\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.local_variables_initializer())\n",
    "    \n",
    "    train_loss, train_accuracy = [], []\n",
    "    valid_loss, valid_accuracy = [], []\n",
    "    \n",
    "    num_train_batches = int(x.shape[0] / batch_size)\n",
    "    \n",
    "    # Training\n",
    "    print(\"Beginning Training...\")\n",
    "    for e in range(epochs):\n",
    "        sum_loss, sum_accuracy = 0, 0\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(_batches([x, y], batch_size=batch_size, shuffle=shuffle, allow_smaller_final_batch=False)):\n",
    "            batch_x = np.expand_dims(batch_x, 1)\n",
    "            batch_x = np.stack([augment.augment_video(batch_x[i], session, augment_op) for i in range(batch_size)])\n",
    "            batch_x = np.squeeze(batch_x, 1)\n",
    "            \n",
    "            loss, accuracy, _ = session.run([loss_op, accuracy_op, train_op],\n",
    "                                            feed_dict={X: batch_x, Y: batch_y, training: True})\n",
    "\n",
    "            if verbose and batch_i % print_interval == 0:\n",
    "                print(\"Train Batch {}: Loss = {}, Accuracy = {}\".format(batch_i, loss, accuracy))\n",
    "            sum_loss += loss\n",
    "            sum_accuracy += accuracy\n",
    "\n",
    "        train_loss.append(sum_loss / num_train_batches)\n",
    "        train_accuracy.append(sum_accuracy / num_train_batches)\n",
    "        if verbose:\n",
    "            print(\"Epoch {}: Average Train Loss = {}, Average Train Accuracy = {}\\n\"\n",
    "                  .format(e + 1, train_loss[e], train_accuracy[e]))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "    plt.plot(np.arange(epochs), np.array(train_loss), label=\"Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(np.arange(epochs), np.arange(epochs))\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__int__ returned non-int (type NoneType)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a065b1bbe65e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_history_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m45\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mfit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\School\\CS175\\Project\\github\\CS175_RPS_AI\\models.py\u001b[0m in \u001b[0;36mmodel1\u001b[1;34m(image_size, image_history_length)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mconc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __int__ returned non-int (type NoneType)"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as session:\n",
    "    input_size = [64, 64, 1]\n",
    "    model1 = models.model1(input_size, image_history_length=45)\n",
    "    fit1(session, model1, X1, Y1, epochs=3, shuffle=True, verbose=True, print_interval=10)\n",
    "    model2 = models.model2(input_size)\n",
    "    fit2(session, model2, X2, Y2, epochs=3, shuffle=True, verbose=True, print_interval=10)\n",
    "    \n",
    "    # Save Model\n",
    "    # https://www.tensorflow.org/programmers_guide/saved_model\n",
    "    save_path = os.path.join(os.getcwd(), \"savedmodels\\\\both\\\\models.ckpt\")\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session, save_path)\n",
    "    print(\"Session Saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Cross-Validation that uses Custom Batch Generator\n",
    "def cross_validation1(session, model, x, y, batch_size=64, epochs=5, K=5, shuffle=False, verbose=True, print_interval=100):\n",
    "    # https://stackoverflow.com/questions/39748660/how-to-perform-k-fold-cross-validation-with-tensorflow\n",
    "    [predict_op, loss_op, accuracy_op, train_op], (X, Y, training) = model\n",
    "\n",
    "    # K-Fold Loop\n",
    "    train_loss, train_accuracy = [], []\n",
    "    valid_loss, valid_accuracy = [], []\n",
    "    k = 0\n",
    "    for train_i, valid_i in KFold(n_splits=K).split(x):\n",
    "        train_loss.append([])\n",
    "        train_accuracy.append([])\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.local_variables_initializer())\n",
    "\n",
    "        num_train_batches = int(x[train_i].shape[0] / batch_size)\n",
    "        num_valid_batches = int(x[valid_i].shape[0] / batch_size)\n",
    "\n",
    "        # Training\n",
    "        for e in range(epochs):\n",
    "            sum_loss, sum_accuracy = 0, 0\n",
    "            for batch_i, (batch_x, batch_y) in enumerate(_batches_video([x[train_i], y[train_i]], batch_size=batch_size, shuffle=shuffle, allow_smaller_final_batch=False)):\n",
    "                batch_y = batch_y[:, -1]\n",
    "                loss, accuracy, _ = session.run([loss_op, accuracy_op, train_op],\n",
    "                                                feed_dict={X: batch_x, Y: batch_y, training: True})\n",
    "\n",
    "                if verbose and batch_i % print_interval == 0:\n",
    "                    print(\"Train Batch {}: Loss = {}, Accuracy = {}\".format(batch_i, loss, accuracy))\n",
    "                sum_loss += loss\n",
    "                sum_accuracy += accuracy\n",
    "\n",
    "            train_loss[k].append(sum_loss / num_train_batches)\n",
    "            train_accuracy[k].append(sum_accuracy / num_train_batches)\n",
    "            if verbose:\n",
    "                print(\"Epoch {}: Average Train Loss = {}, Average Train Accuracy = {}\\n\"\n",
    "                      .format(e + 1, train_loss[k][e], train_accuracy[k][e]))\n",
    "\n",
    "        # Validation\n",
    "        sum_loss, sum_accuracy = 0, 0\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(_batches_video([x[valid_i], y[valid_i]], batch_size=batch_size, shuffle=shuffle, allow_smaller_final_batch=False)):\n",
    "            batch_y = batch_y[:, -1]\n",
    "            loss, accuracy = session.run([loss_op, accuracy_op],\n",
    "                                         feed_dict={X: batch_x, Y: batch_y, training: False})\n",
    "\n",
    "            if verbose and batch_i % print_interval == 0:\n",
    "                print(\"Valid Batch {}: Loss = {}, Accuracy = {}\".format(batch_i, loss, accuracy))\n",
    "            sum_loss += loss\n",
    "            sum_accuracy += accuracy\n",
    "\n",
    "        valid_loss.append(sum_loss / num_valid_batches)\n",
    "        valid_accuracy.append(sum_accuracy / num_valid_batches)\n",
    "        if verbose:\n",
    "            print(\"Fold {}: Validation Loss = {}, Validation Accuracy = {}\\n\"\n",
    "                  .format(k + 1, valid_loss[k], valid_accuracy[k]))\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    # Results\n",
    "    print(\"Average Valid Loss = {}, Average Valid Accuracy = {}\".format(mean(valid_loss), mean(valid_accuracy)))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "    plt.plot(np.arange(epochs), np.array(train_loss).T)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(np.arange(epochs), np.arange(epochs))\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Fold %d\" % i for i in range(1, K+1)])\n",
    "    plt.show()\n",
    "    \n",
    "def cross_valididation2(session, model, x, y, batch_size=64, epochs=5, K=5, shuffle=False, verbose=True, print_interval=100):\n",
    "    # https://stackoverflow.com/questions/39748660/how-to-perform-k-fold-cross-validation-with-tensorflow\n",
    "    [predict_op, loss_op, accuracy_op, train_op], (X, Y, training) = model\n",
    "\n",
    "    # K-Fold Loop\n",
    "    train_loss, train_accuracy = [], []\n",
    "    valid_loss, valid_accuracy = [], []\n",
    "    k = 0\n",
    "    for train_i, valid_i in KFold(n_splits=K).split(x):\n",
    "        train_loss.append([])\n",
    "        train_accuracy.append([])\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.local_variables_initializer())\n",
    "\n",
    "        num_train_batches = int(x[train_i].shape[0] / batch_size)\n",
    "        num_valid_batches = int(x[valid_i].shape[0] / batch_size)\n",
    "\n",
    "        # Training\n",
    "        for e in range(epochs):\n",
    "            sum_loss, sum_accuracy = 0, 0\n",
    "            for batch_i, (batch_x, batch_y) in enumerate(_batches([x[train_i], y[train_i]], batch_size=batch_size, shuffle=shuffle, allow_smaller_final_batch=False)):\n",
    "                loss, accuracy, _ = session.run([loss_op, accuracy_op, train_op],\n",
    "                                                feed_dict={X: batch_x, Y: batch_y, training: True})\n",
    "\n",
    "                if verbose and batch_i % print_interval == 0:\n",
    "                    print(\"Train Batch {}: Loss = {}, Accuracy = {}\".format(batch_i + 1, loss, accuracy))\n",
    "                sum_loss += loss\n",
    "                sum_accuracy += accuracy\n",
    "\n",
    "            train_loss[k].append(sum_loss / num_train_batches)\n",
    "            train_accuracy[k].append(sum_accuracy / num_train_batches)\n",
    "            if verbose:\n",
    "                print(\"Epoch {}: Average Train Loss = {}, Average Train Accuracy = {}\\n\"\n",
    "                      .format(e + 1, train_loss[k][e], train_accuracy[k][e]))\n",
    "\n",
    "        # Validation\n",
    "        sum_loss, sum_accuracy = 0, 0\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(_batches([x[valid_i], y[valid_i]], batch_size=batch_size, shuffle=shuffle, allow_smaller_final_batch=False)):\n",
    "            loss, accuracy = session.run([loss_op, accuracy_op],\n",
    "                                         feed_dict={X: batch_x, Y: batch_y, training: False})\n",
    "\n",
    "            if verbose and batch_i % print_interval == 0:\n",
    "                print(\"Valid Batch {}: Loss = {}, Accuracy = {}\".format(batch_i + 1, loss, accuracy))\n",
    "            sum_loss += loss\n",
    "            sum_accuracy += accuracy\n",
    "\n",
    "        valid_loss.append(sum_loss / num_valid_batches)\n",
    "        valid_accuracy.append(sum_accuracy / num_valid_batches)\n",
    "        if verbose:\n",
    "            print(\"Fold {}: Validation Loss = {}, Validation Accuracy = {}\\n\"\n",
    "                  .format(k + 1, valid_loss[k], valid_accuracy[k]))\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    # Results\n",
    "    print(\"Average Valid Loss = {}, Average Valid Accuracy = {}\".format(mean(valid_loss), mean(valid_accuracy)))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "    plt.plot(np.arange(epochs), np.array(train_loss).T)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(np.arange(epochs), np.arange(epochs))\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Fold %d\" % i for i in range(1, K+1)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graphs()\n",
    "with tf.Session() as session:\n",
    "    image_shape = X1.shape[1:]\n",
    "    model = models.model1(image_shape)\n",
    "    cross_validation1(model1, session, X1, Y1, epochs=5, shuffle=True, print_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graphs()\n",
    "with tf.Session() as session:\n",
    "    image_shape = X2.shape[1:]\n",
    "    mode2 = models.model2(image_shape)\n",
    "    cross_validation2(model2, session, X2, Y2, epochs=5, shuffle=True, print_interval=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
